{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.70 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.61  Python-3.11.11 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset/data.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning G:\\Projects\\Saffron_Detection\\dataset\\train\\labels.cache... 1029 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1029/1029 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Projects\\Saffron_Detection\\dataset\\valid\\labels.cache... 98 images, 0 backgrounds, 0 corrupt: 100%|██████████| 98/98 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      2.27G      1.472      2.102       1.61         14        640: 100%|██████████| 65/65 [00:21<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.574      0.347       0.42      0.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      2.25G      1.479      1.758      1.616         15        640: 100%|██████████| 65/65 [00:17<00:00,  3.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.575      0.653      0.632      0.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      2.24G      1.459      1.559      1.585         25        640: 100%|██████████| 65/65 [00:17<00:00,  3.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.666      0.653      0.685      0.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      2.25G      1.392      1.384      1.546         15        640: 100%|██████████| 65/65 [00:18<00:00,  3.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.778      0.642      0.724      0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      2.26G      1.312      1.217      1.451         24        640: 100%|██████████| 65/65 [00:17<00:00,  3.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.831      0.718      0.856      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.61  Python-3.11.11 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.831      0.718      0.856      0.517\n",
      "Speed: 0.7ms preprocess, 4.9ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model on our dataset\n",
    "trained_model = model.train(\n",
    "    data = 'dataset/data.yaml', \n",
    "    epochs = 5, \n",
    "    project = 'runs'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.61  Python-3.11.11 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Projects\\Saffron_Detection\\dataset\\valid\\labels.cache... 98 images, 0 backgrounds, 0 corrupt: 100%|██████████| 98/98 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:17<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        245      0.754      0.776      0.855      0.517\n",
      "Speed: 0.9ms preprocess, 9.6ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Validating the model on valid images and calculating metrics\n",
    "validations = YOLO('runs/train/weights/best.pt')\n",
    "metrics = model.val(\n",
    "    source='data.yaml', \n",
    "    project='runs'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing/Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\102_jpg.rf.68f04a1fa860bb1ac9a3f4919b151b37.jpg: 640x640 21 Saffrons, 59.9ms\n",
      "image 2/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\114_jpg.rf.fa5c35074e57f57e0dee5a349487d0af.jpg: 640x640 1 Saffron, 65.9ms\n",
      "image 3/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\129_jpg.rf.0651379ba0ce3f1018165db63d47c7b6.jpg: 640x640 5 Saffrons, 67.8ms\n",
      "image 4/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\132_jpg.rf.ac242f0c841b392ad9e1e786f55d2762.jpg: 640x640 3 Saffrons, 60.1ms\n",
      "image 5/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\137_jpg.rf.3d30de2da44fa3ad3530e835139410e5.jpg: 640x640 3 Saffrons, 58.3ms\n",
      "image 6/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\13_jpg.rf.c93b2a3af5e7de7f847792f41fb2bec5.jpg: 640x640 1 Saffron, 57.5ms\n",
      "image 7/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\141_jpg.rf.92691570cef7f2ce69b918f895232a60.jpg: 640x640 1 Saffron, 40.9ms\n",
      "image 8/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\146_jpg.rf.74503d8a0476936d5fef8715d4f913a5.jpg: 640x640 2 Saffrons, 46.3ms\n",
      "image 9/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\151_jpg.rf.ae9be291f6868e1852b1120e7393e270.jpg: 640x640 1 Saffron, 27.9ms\n",
      "image 10/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\154_jpg.rf.14e28bcd6f0e6a0cc9ec0b0052ff9501.jpg: 640x640 1 Saffron, 31.1ms\n",
      "image 11/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\182_jpg.rf.3ee33b71699e66c55c0b95f4e1ab9f19.jpg: 640x640 1 Saffron, 28.5ms\n",
      "image 12/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\201_jpg.rf.800cdd9ef9a4e8a0dd5062209cae37a7.jpg: 640x640 1 Saffron, 36.5ms\n",
      "image 13/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\212_jpg.rf.28035942fc2052f625ffd6c51dc1f20f.jpg: 640x640 1 Saffron, 40.2ms\n",
      "image 14/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\217_jpg.rf.dd669b1b2f6c09f00a882a1d42d721a0.jpg: 640x640 2 Saffrons, 40.4ms\n",
      "image 15/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\221_jpg.rf.de01e92a55eb51b81af7f738b1258d6a.jpg: 640x640 1 Saffron, 27.8ms\n",
      "image 16/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\232_jpg.rf.816b77b107667cd4615b8eca11287bf5.jpg: 640x640 3 Saffrons, 38.6ms\n",
      "image 17/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\236_jpg.rf.741b551df0b30041f62cb696d6b211fe.jpg: 640x640 2 Saffrons, 27.5ms\n",
      "image 18/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\238_jpg.rf.b6456b648baa9b4ee5cd83676a3ecff0.jpg: 640x640 1 Saffron, 25.9ms\n",
      "image 19/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\239_jpg.rf.18f72ffb578871834313fe0e31e451de.jpg: 640x640 2 Saffrons, 27.9ms\n",
      "image 20/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\269_jpg.rf.84482d49e2f5142d85ff49176ecc4a2a.jpg: 640x640 9 Saffrons, 27.6ms\n",
      "image 21/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\279_jpg.rf.0690cb4cc7aff18778c4e9ea5128fd0a.jpg: 640x640 2 Saffrons, 26.7ms\n",
      "image 22/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\292_jpg.rf.ed399fbbea5aef12594443617c036b24.jpg: 640x640 1 Saffron, 24.8ms\n",
      "image 23/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\294_jpg.rf.53ae48de61da195323980b3db6ae2ad9.jpg: 640x640 5 Saffrons, 26.0ms\n",
      "image 24/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\29_jpg.rf.5789bf6f56e5cae277b7b17a131a87f4.jpg: 640x640 3 Saffrons, 16.2ms\n",
      "image 25/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\303_jpg.rf.5661b6d5859a8005a1998f46f9c87fcb.jpg: 640x640 3 Saffrons, 16.8ms\n",
      "image 26/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\308_jpg.rf.17112dc96cd551069ebd86a142c29eca.jpg: 640x640 1 Saffron, 23.8ms\n",
      "image 27/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\30_jpg.rf.8b75f1155c9fa192c097e8053553cfb8.jpg: 640x640 1 Saffron, 21.8ms\n",
      "image 28/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\322_jpg.rf.57647dd52b57f6ffbcea691c893100b7.jpg: 640x640 8 Saffrons, 28.5ms\n",
      "image 29/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\370_jpg.rf.e8e40b0dff1e73171169e8a85daa904c.jpg: 640x640 3 Saffrons, 21.0ms\n",
      "image 30/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\378_jpg.rf.57733426882af2511087a6eda7b2823c.jpg: 640x640 6 Saffrons, 13.1ms\n",
      "image 31/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\383_jpg.rf.197252ce1bed22547246db1fd7677e80.jpg: 640x640 4 Saffrons, 10.4ms\n",
      "image 32/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\386_jpg.rf.8d0fd9bfddd70c457795a2a66b85d23c.jpg: 640x640 8 Saffrons, 22.0ms\n",
      "image 33/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\38_jpg.rf.0e5a8fa0e72c30c5635f197dc9d1ea73.jpg: 640x640 2 Saffrons, 18.5ms\n",
      "image 34/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\390_jpg.rf.cbbdb338bbfe3624bf4837ea34779127.jpg: 640x640 1 Saffron, 24.7ms\n",
      "image 35/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\399_jpg.rf.181ec6f17ae43f996b61c732f9537b35.jpg: 640x640 1 Saffron, 19.1ms\n",
      "image 36/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\403_jpg.rf.e8b8add28bca898b5287b802c69b602c.jpg: 640x640 3 Saffrons, 44.8ms\n",
      "image 37/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\409_jpg.rf.b6d877bec2ffbb6a0b32a727063c285e.jpg: 640x640 3 Saffrons, 30.3ms\n",
      "image 38/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\419_jpg.rf.0fdae00e31eaf2003fc3b44826b1de53.jpg: 640x640 2 Saffrons, 30.9ms\n",
      "image 39/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\431_jpg.rf.75c865adfe6fe4c9f55c0cbcb654bf92.jpg: 640x640 1 Saffron, 19.6ms\n",
      "image 40/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\437_jpg.rf.0ed6e9d2ffc250212e68f6af99efccf3.jpg: 640x640 3 Saffrons, 19.2ms\n",
      "image 41/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\439_jpg.rf.b340445f0becaeab21494d87a3b194a7.jpg: 640x640 3 Saffrons, 16.5ms\n",
      "image 42/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\452_jpg.rf.81ece4ae4b3a601dbb96b020febede86.jpg: 640x640 4 Saffrons, 24.0ms\n",
      "image 43/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\493_jpg.rf.19575d77ec4b102d6104961437a2ac9a.jpg: 640x640 2 Saffrons, 22.8ms\n",
      "image 44/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\57_jpg.rf.723cb7cd8c4f447b60d714fe8207598b.jpg: 640x640 3 Saffrons, 22.2ms\n",
      "image 45/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\63_jpg.rf.fb1c6784639d0c1ddc21fccbaa7feec9.jpg: 640x640 3 Saffrons, 25.9ms\n",
      "image 46/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\72_jpg.rf.931db32e8e9c04f718fe1a6828a96253.jpg: 640x640 3 Saffrons, 28.3ms\n",
      "image 47/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\75_jpg.rf.2ddcd47099830d8730da46388c697c11.jpg: 640x640 2 Saffrons, 35.1ms\n",
      "image 48/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\76_jpg.rf.9f40fce03b328e5d30426f86daf58a67.jpg: 640x640 7 Saffrons, 23.1ms\n",
      "image 49/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\79_jpg.rf.5be050af653b9e2eacc9c63b8e4a735a.jpg: 640x640 2 Saffrons, 20.4ms\n",
      "image 50/50 g:\\Projects\\Saffron_Detection\\dataset\\test\\images\\92_jpg.rf.ae8f4a5ccffccdd12a313706dc7fecbb.jpg: 640x640 7 Saffrons, 19.8ms\n",
      "Speed: 23.3ms preprocess, 30.7ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test images\n",
    "predictions = model.predict(\n",
    "    source = 'dataset/test/images', \n",
    "    save=True, \n",
    "    project='runs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
